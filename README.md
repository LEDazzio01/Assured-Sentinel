# ğŸ›¡ï¸ Assured Sentinel
**Probabilistic Guardrails for Generative Code**

---

## ğŸš€ The Mission
The rapid ascendancy of Large Language Models (LLMs) has introduced a profound challenge: the inherent uncertainty of probabilistic outputs. While LLMs accelerate development, they cannot inherently quantify the risk of the code they generate.

**Assured Sentinel** bridges the gap between stochastic generation and deterministic safety. It is not merely a linter; it is a system that imposes mathematical safety guarantees on generative models using **Split Conformal Prediction (SCP).**

---

## ğŸ›ï¸ Architectural Overview
The system employs a **Twoâ€‘Agent Pattern** to decouple generation from verification, preventing the selfâ€‘delusion common in singleâ€‘agent loops.

### ğŸ§  The Analyst (Generator)
- **Role:** The stochastic motor  
- **Behavior:** Highâ€‘temperature (0.8), highâ€‘entropy generation  
- **Objective:** Propose creative solutions to user queries  
- **Stack:** Azure OpenAI (gptâ€‘4o) via Semantic Kernel  

### ğŸ›¡ï¸ The Commander (Guardrail)
- **Role:** The deterministic logic gate  
- **Behavior:** Strictly procedural and statistical  
- **Mechanism:**  
  - Calculates a Nonâ€‘Conformity Score (Inverse Security Score) using static analysis (simulated or bandit).  
  - Compares against a preâ€‘calibrated threshold (\( \hat{q} \)).  
- **Guarantee:** Ensures accepted code meets a specific risk tolerance (\(1 - \alpha\)) with statistical validity.  

---

## ğŸ› ï¸ Quick Start

### Prerequisites
- Python 3.10+  
- Azure OpenAI API Key  

### Installation
```bash
# Clone the repository
git clone https://github.com/your-username/assured-sentinel.git
cd assured-sentinel
``` 

# Install dependencies
```
pip install -r requirements.txt
```


# Configuration

Create a .env file in the root directory
```
AZURE_OPENAI_DEPLOYMENT_NAME="gpt-4o"
AZURE_OPENAI_ENDPOINT="https://your-resource.openai.azure.com/"
AZURE_OPENAI_API_KEY="your-key-here"
```

# Calibration (Oneâ€‘Time Setup)

Run calibration to establish safety threshold (q_hat)
```
python calibration.py
```

Output: Generates calibration_data.pkl containing the calculated safety threshold based on (\alpha = 0.1).

# Run the Sentinel

Execute the main event loop
```
python run_day4.py
```

ğŸ“‚ Project Structure
```
assured-sentinel/
â”œâ”€â”€ analyst.py       # LLM Agent (Azure OpenAI + Semantic Kernel)
â”œâ”€â”€ commander.py     # Logic Gate (Conformal Prediction Verifier)
â”œâ”€â”€ calibration.py   # Calibration script (calculates q_hat from MBPP)
â”œâ”€â”€ scorer.py        # Non-Conformity Measure (NCM) logic
â”œâ”€â”€ run_day4.py      # Main entry point / orchestration loop
â”œâ”€â”€ requirements.txt # Project dependencies
â””â”€â”€ README.md        # Documentation
```

# ğŸ“ Theoretical Framework

We strictly adhere to Split Conformal Prediction (SCP).

 - Calibration Set: A "Ground Truth" dataset (e.g., MBPP) establishes a baseline distribution of code scores.

 - Nonâ€‘Conformity Measure (NCM): A scoring function where 0.0 = Secure and 1.0 = Vulnerable.

# The Guarantee:

[ P(Y_{n+1} \in C(X_{n+1})) \geq 1 - \alpha ]

We target (\alpha = 0.1), providing 90% confidence that accepted code belongs to the set of secure outputs.

# ğŸ“Š Roadmap

 [x] Phase 1 â€“ Build (MVP):

 - Implement Analyst/Commander MAS pattern

 - Implement NCM scoring logic

 - Establish calibration set with MBPP

[ ] Phase 2 â€“ Validate:

 - Benchmark SCP thresholds across diverse datasets

 - Stressâ€‘test against adversarial prompts

[ ] Phase 3 â€“ Deploy:

 - Package as reproducible GitHub Codespace

 - Integrate with CI/CD pipelines

# ğŸ“– References

 - Practical Applied Conformal Prediction by Valeriy Manokhin

 - HumanEval & MBPP datasets

 - Bandit: Python Security Linter
